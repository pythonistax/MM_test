{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8964a302",
   "metadata": {},
   "source": [
    "\n",
    "General DEPREC Code - Shared Utility Functions\n",
    "\n",
    "This module contains common utility functions used across the DEPREC reconciliation system.\n",
    "These functions handle data cleaning, type conversion, and common data transformations.\n",
    "\n",
    "## ***Section 1 - Imports and Configuration***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4133474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Telegram (optional - only if sending messages)\n",
    "from telegram import Bot\n",
    "from telegram.constants import ParseMode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2fc979",
   "metadata": {},
   "source": [
    "## **2) Data Import & Cleanup - Getting to Central Bank Statement DataFrame**\n",
    "\n",
    "This section handles the complete pipeline from raw bank statement files to a unified `central_df`:\n",
    "\n",
    "### **Steps:**\n",
    "1. **Import raw bank statements** - Automatically loads all CSV/Excel files from directory (excludes GWID/Rubric files)\n",
    "   - Cleans numeric columns (removes `$`, `%`, commas)\n",
    "   - Converts date columns to datetime format\n",
    "   - Removes \"Beginning balance\" rows\n",
    "   - Drops blank rows/columns and \"Unnamed\" columns\n",
    "\n",
    "2. **Detect bank type** - Identifies whether each file is Chase, BofA, or Unknown based on column structure\n",
    "   - Chase: `['Details', 'Posting Date', 'Description', 'Amount', 'Type']`\n",
    "   - BofA: `['Date', 'Description', 'Amount']`\n",
    "\n",
    "3. **Split by bank & validate** - Separates files by bank type, concatenates each bank's files\n",
    "   - Creates `chase_central` and `bofa_central` DataFrames\n",
    "   - Validates row counts, column structure, date ranges, and amount totals\n",
    "\n",
    "4. **Standardize BofA format** - Transforms `bofa_central` to match Chase column structure\n",
    "   - Maps `Date` → `Posting Date`\n",
    "   - Derives `Details` from amount sign (negative = DEBIT, positive = CREDIT)\n",
    "   - Adds `Type` column with `ACH_` prefix\n",
    "\n",
    "5. **Create unified central_df** - Concatenates `chase_central` + `bofa_central_standardized`\n",
    "   - **Output:** Single DataFrame (`central_df`) with all bank statements in standardized format\n",
    "   - Columns: `['Details', 'Posting Date', 'Description', 'Amount', 'Type']`\n",
    "\n",
    "**Note:** AI-powered bank detection functions are defined but not yet integrated into the main pipeline.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e753b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removed 1 row(s) containing 'Beginning balance'\n",
      "Index(['Gateway ID', 'Gateway', 'Provider Name', 'Currency',\n",
      "       'Global Monthly Cap', 'Transactions', 'Orders',\n",
      "       'Gross Approved Revenue', 'Refund - Partial #', 'Refund - Full #',\n",
      "       'Void Refund Amount', 'Total $', 'Chargeback #', 'Chargeback Lost Rev',\n",
      "       'Chargeback %', 'Decline #', 'Decline %', 'Pending #', 'Pending $',\n",
      "       'Customer Approval %', 'Affiliate Breakdown'],\n",
      "      dtype='object')\n",
      "Index(['details', 'postingdate', 'description', 'amount', 'type'], dtype='object')\n",
      "Index(['Details', 'Posting Date', 'Description', 'Amount', 'Type'], dtype='object')\n",
      "Index(['Details', 'Posting Date', 'Description', 'Amount', 'Type'], dtype='object')\n",
      "Index(['Details', 'Posting Date', 'Description', 'Amount', 'Type'], dtype='object')\n",
      "Index(['Details', 'Posting Date', 'Description', 'Amount', 'Type'], dtype='object')\n",
      "Index(['Details', 'Posting Date', 'Description', 'Amount', 'Type'], dtype='object')\n",
      "Index(['Unnamed 0', 'Gateway Id', 'Gateway', 'Provider Name', 'Currency',\n",
      "       'Global Monthly Cap', 'Transactions', 'Orders',\n",
      "       'Gross Approved Revenue', 'Refund Partial', 'Refund Full',\n",
      "       'Void Refund Amount', 'Total', 'Chargeback', 'Chargeback Lost Rev',\n",
      "       'Chargeback2', 'Decline', 'Decline2', 'Pending', 'Pending2',\n",
      "       'Customer Approval', 'Affiliate Breakdown', 'Processor Name', 'Corp',\n",
      "       'Occurred Deposits', 'Total Revenue w/o Reserves',\n",
      "       'Total Revenue w/ Reserves', 'Difference w/o Reserves',\n",
      "       'Difference w/ Reserves'],\n",
      "      dtype='object')\n",
      "Index(['Date', 'Description', 'Amount'], dtype='object')\n",
      "Index(['Gateway ID', 'Gateway', 'Provider Name', 'Currency',\n",
      "       'Global Monthly Cap', 'Transactions', 'Orders',\n",
      "       'Gross Approved Revenue', 'Refund - Partial #', 'Refund - Full #',\n",
      "       'Void Refund Amount', 'Total $', 'Chargeback #', 'Chargeback Lost Rev',\n",
      "       'Chargeback %', 'Decline #', 'Decline %', 'Pending #', 'Pending $',\n",
      "       'Customer Approval %', 'Affiliate Breakdown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "def _drop_fully_blank_and_unnamed(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove columns that are either all blank/NaN or whose names are Unnamed: ... (case-insensitive).\n",
    "    \"\"\"\n",
    "    def is_unnamed(col):\n",
    "        col_str = str(col).strip().lower()\n",
    "        return col_str.startswith(\"unnamed:\") or col_str == ''\n",
    "    # First drop full blank columns\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    # Then drop columns that are unnamed\n",
    "    cols_to_drop = [col for col in df.columns if is_unnamed(col)]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    return df\n",
    "\n",
    "def _drop_fully_blank_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes rows that are entirely blank/NaN.\n",
    "    \"\"\"\n",
    "    return df.dropna(axis=0, how='all')\n",
    "\n",
    "def _remove_beginning_balance_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows that contain 'Beginning balance' in any column (case-insensitive).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to clean\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with 'Beginning balance' rows removed\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # Check each row to see if any column contains 'Beginning balance'\n",
    "    mask = df_cleaned.astype(str).apply(\n",
    "        lambda row: any('beginning balance' in str(val).lower() for val in row), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove rows where mask is True\n",
    "    rows_removed = mask.sum()\n",
    "    if rows_removed > 0:\n",
    "        df_cleaned = df_cleaned[~mask]\n",
    "        print(f\"  Removed {rows_removed} row(s) containing 'Beginning balance'\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def _clean_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean numeric columns by removing $, %, and commas, then convert to numeric.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to clean\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with cleaned numeric columns\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    for col in df_cleaned.columns:\n",
    "        # Check if column is string/object type (might contain numeric data with formatting)\n",
    "        if df_cleaned[col].dtype == 'object' or df_cleaned[col].dtype == 'string':\n",
    "            # Try to detect if it looks numeric (contains $, %, or numbers with commas)\n",
    "            sample_values = df_cleaned[col].dropna().astype(str).head(10)\n",
    "            looks_numeric = any(\n",
    "                '$' in str(val) or '%' in str(val) or ',' in str(val) \n",
    "                for val in sample_values\n",
    "            )\n",
    "            \n",
    "            if looks_numeric:\n",
    "                # Remove $, %, and commas, then convert to numeric\n",
    "                df_cleaned[col] = (\n",
    "                    df_cleaned[col]\n",
    "                    .astype(str)\n",
    "                    .str.replace('$', '', regex=False)\n",
    "                    .str.replace('%', '', regex=False)\n",
    "                    .str.replace(',', '', regex=False)\n",
    "                    .str.strip()\n",
    "                )\n",
    "                # Convert to numeric, coercing errors to NaN\n",
    "                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def _clean_date_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert columns with 'Date' in the name to datetime format.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to clean\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with date columns converted to datetime\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    for col in df_cleaned.columns:\n",
    "        # Check if column name contains 'date' (case-insensitive)\n",
    "        if 'date' in str(col).lower():\n",
    "            try:\n",
    "                # Convert to datetime, coercing errors to NaT\n",
    "                df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce')\n",
    "            except Exception as e:\n",
    "                # If conversion fails, leave as is and print warning\n",
    "                print(f\"Warning: Could not convert column '{col}' to datetime: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def bank_statements_retriever(directory: Optional[str] = None) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Scan a directory for Excel (.xlsx, .xls) and CSV (.csv) files.\n",
    "    Excludes files containing 'GWID' or 'Rubric' in the filename (case-insensitive).\n",
    "    Returns a dict mapping filename to DataFrame.\n",
    "    \n",
    "    During import, automatically cleans:\n",
    "    - Numeric columns: removes $, %, and commas, then converts to numeric\n",
    "    - Date columns: converts columns with 'Date' in name to datetime format\n",
    "    - Removes rows containing 'Beginning balance' in any column\n",
    "\n",
    "    Args:\n",
    "        directory: Path to the folder to scan. Defaults to current working directory.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Keys are filenames -> DataFrame values\n",
    "    \"\"\"\n",
    "    directory = directory or os.getcwd()\n",
    "    all_files: List[str] = os.listdir(directory)\n",
    "\n",
    "    # Identify candidate files (exclude GWID and Rubric files)\n",
    "    candidates: List[str] = []\n",
    "    for file_name in all_files:\n",
    "        lower_name = file_name.lower()\n",
    "        # Check if file is Excel or CSV\n",
    "        is_excel_or_csv = (\n",
    "            lower_name.endswith(\".csv\")\n",
    "            or lower_name.endswith(\".xlsx\")\n",
    "            or lower_name.endswith(\".xls\")\n",
    "        )\n",
    "        # Exclude files with 'gwid' or 'rubric' in the name\n",
    "        has_gwid = \"gwid\" in lower_name\n",
    "        has_rubric = \"rubric\" in lower_name\n",
    "        \n",
    "        if is_excel_or_csv and not has_gwid and not has_rubric:\n",
    "            candidates.append(file_name)\n",
    "\n",
    "    # Load dataframes with filename as key\n",
    "    dataframes: Dict[str, pd.DataFrame] = {}\n",
    "    for file_name in candidates:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        ext = os.path.splitext(file_name)[1].lower()\n",
    "\n",
    "        try:\n",
    "            if ext == \".csv\":\n",
    "                df = pd.read_csv(file_path)\n",
    "            elif ext in (\".xlsx\", \".xls\"):\n",
    "                df = pd.read_excel(file_path)\n",
    "            else:\n",
    "                continue  # Should not happen due to filter, but guard anyway\n",
    "\n",
    "            # Clean numeric columns (remove $, %, commas and convert to numeric)\n",
    "            df = _clean_numeric_columns(df)\n",
    "            \n",
    "            # Clean date columns (convert columns with 'Date' in name to datetime)\n",
    "            df = _clean_date_columns(df)\n",
    "            \n",
    "            # Remove rows containing 'Beginning balance'\n",
    "            df = _remove_beginning_balance_rows(df)\n",
    "            \n",
    "            # Clean fully blank columns and \"Unnamed\" columns\n",
    "            df = _drop_fully_blank_and_unnamed(df)\n",
    "            # Drop fully blank rows\n",
    "            df = _drop_fully_blank_rows(df)\n",
    "\n",
    "            # If after dropping, the DataFrame is empty (no rows), skip adding\n",
    "            if df.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            # Use filename as key (not modified)\n",
    "            dataframes[file_name] = df\n",
    "            \n",
    "        except Exception as exc:\n",
    "            print(f\"Warning: Failed to read file '{file_name}': {exc}\")\n",
    "            continue\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "x = bank_statements_retriever()\n",
    "for name, df in x.items():\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0e0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bank_selector(dataframes_dict: Dict[str, pd.DataFrame]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Bank Selector Function\n",
    "    \n",
    "    This function analyzes a dictionary of DataFrames (where keys are filenames and values are the DataFrames)\n",
    "    to identify which bank each file belongs to based on column structure.\n",
    "    \n",
    "    For each DataFrame:\n",
    "    - Cleans column names (lowercase, strip whitespace)\n",
    "    - Checks if columns match Chase bank statement format: ['details', 'posting date', 'description', 'amount', 'type']\n",
    "    - Checks if columns match BofA bank statement format: ['date', 'description', 'amount']\n",
    "    - Returns a dictionary mapping filename to bank name\n",
    "    \n",
    "    Args:\n",
    "        dataframes_dict: Dictionary where keys are filenames and values are pandas DataFrames\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary mapping filename -> bank name (e.g., \"Chase\", \"BofA\", or \"Unknown\")\n",
    "    \"\"\"\n",
    "    bank_mapping: Dict[str, str] = {}\n",
    "    \n",
    "    # Define the expected Chase columns (cleaned and lowercased from: 'Details', 'Posting Date', 'Description', 'Amount', 'Type')\n",
    "    chase_columns = ['details', 'posting date', 'description', 'amount', 'type']\n",
    "    \n",
    "    # Define the expected BofA columns (cleaned and lowercased from: 'Date', 'Description', 'Amount')\n",
    "    bofa_columns = ['date', 'description', 'amount']\n",
    "    \n",
    "    for filename, df in dataframes_dict.items():\n",
    "        # Clean column names: lowercase and strip whitespace\n",
    "        cleaned_columns = [col.strip().lower() for col in df.columns]\n",
    "        \n",
    "        # Check if columns match Chase format\n",
    "        if sorted(cleaned_columns) == sorted(chase_columns):\n",
    "            bank_mapping[filename] = \"Chase\"\n",
    "        # Check if columns match BofA format\n",
    "        elif sorted(cleaned_columns) == sorted(bofa_columns):\n",
    "            bank_mapping[filename] = \"BofA\"\n",
    "        else:\n",
    "            bank_mapping[filename] = \"Unknown\"\n",
    "    \n",
    "    return bank_mapping\n",
    "\n",
    "dataframe_files = bank_selector(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c583cc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validating Chase ===\n",
      "✓ Row count: 2247 rows match\n",
      "✓ Column names: 5 columns match\n",
      "✓ Date column 'Posting Date': Min (2025-09-02 00:00:00) and Max (2025-10-06 00:00:00) match\n",
      "✓ Amount column 'Amount': Total (840093.47) and Std Dev (321.91) match\n",
      "✓ All Chase validation checks passed!\n",
      "\n",
      "\n",
      "=== Validating BofA ===\n",
      "✓ Row count: 380 rows match\n",
      "✓ Column names: 3 columns match\n",
      "✓ Date column 'Date': Min (2025-07-02 00:00:00) and Max (2025-10-29 00:00:00) match\n",
      "✓ Amount column 'Amount': Total (2851.14) and Std Dev (153.14) match\n",
      "✓ All BofA validation checks passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_and_split_banks(bank_mapping: Dict[str, str], dataframes_dict: Dict[str, pd.DataFrame]) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Filter and Split Banks Function\n",
    "    \n",
    "    This function takes a dictionary mapping filenames to bank names and the original DataFrames dictionary, and:\n",
    "    - Filters to only keep \"Chase\" and \"Bank of America\" (or \"BofA\") entries\n",
    "    - Splits the results into two separate dictionaries: one for Chase, one for BofA\n",
    "    - Concatenates all Chase DataFrames into a single DataFrame (chase_central)\n",
    "    - Concatenates all BofA DataFrames into a single DataFrame (bofa_central)\n",
    "    - Validates with robust checks:\n",
    "        * Row counts match\n",
    "        * Column names match\n",
    "        * Date columns: min and max values match\n",
    "        * Amount columns: totals and standard deviations match\n",
    "    \n",
    "    Args:\n",
    "        bank_mapping: Dictionary where keys are filenames and values are bank names\n",
    "        dataframes_dict: Dictionary where keys are filenames and values are pandas DataFrames\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, any]: Dictionary with four keys:\n",
    "            - \"Chase\": Dictionary of Chase filenames -> bank name\n",
    "            - \"BofA\": Dictionary of Bank of America filenames -> bank name\n",
    "            - \"chase_central\": Concatenated DataFrame of all Chase accounts\n",
    "            - \"bofa_central\": Concatenated DataFrame of all BofA accounts\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any validation checks fail, specifying which checks failed\n",
    "    \"\"\"\n",
    "    chase_files = {}\n",
    "    bofa_files = {}\n",
    "    chase_dataframes = []\n",
    "    bofa_dataframes = []\n",
    "    \n",
    "    # Track row counts for validation\n",
    "    chase_individual_rows = 0\n",
    "    bofa_individual_rows = 0\n",
    "    \n",
    "    for filename, bank_name in bank_mapping.items():\n",
    "        # Normalize bank name for comparison (case-insensitive)\n",
    "        bank_lower = bank_name.lower().strip()\n",
    "        \n",
    "        if bank_lower == \"chase\":\n",
    "            chase_files[filename] = bank_name\n",
    "            # Add the DataFrame to the list for concatenation\n",
    "            if filename in dataframes_dict:\n",
    "                df = dataframes_dict[filename]\n",
    "                chase_dataframes.append(df)\n",
    "                chase_individual_rows += len(df)\n",
    "        elif bank_lower in [\"bofa\", \"bank of america\", \"boa\"]:\n",
    "            bofa_files[filename] = bank_name\n",
    "            # Add the DataFrame to the list for concatenation\n",
    "            if filename in dataframes_dict:\n",
    "                df = dataframes_dict[filename]\n",
    "                bofa_dataframes.append(df)\n",
    "                bofa_individual_rows += len(df)\n",
    "    \n",
    "    # Concatenate all Chase DataFrames\n",
    "    chase_central = pd.concat(chase_dataframes, ignore_index=True) if chase_dataframes else pd.DataFrame()\n",
    "    \n",
    "    # Concatenate all BofA DataFrames\n",
    "    bofa_central = pd.concat(bofa_dataframes, ignore_index=True) if bofa_dataframes else pd.DataFrame()\n",
    "    \n",
    "    # Robust validation function\n",
    "    def validate_concatenation(individual_dfs: List[pd.DataFrame], central_df: pd.DataFrame, bank_name: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Validate concatenation with multiple checks.\n",
    "        Returns list of failed check messages.\n",
    "        \"\"\"\n",
    "        failed_checks = []\n",
    "        passed_checks = []\n",
    "        \n",
    "        if not individual_dfs or central_df.empty:\n",
    "            print(f\"⚠ {bank_name}: No data to validate\")\n",
    "            return failed_checks  # Skip validation if empty\n",
    "        \n",
    "        print(f\"\\n=== Validating {bank_name} ===\")\n",
    "        \n",
    "        # Check 1: Row counts\n",
    "        individual_rows = sum(len(df) for df in individual_dfs)\n",
    "        central_rows = len(central_df)\n",
    "        if individual_rows != central_rows:\n",
    "            failed_checks.append(f\"Row count mismatch: Individual total = {individual_rows}, Central = {central_rows}\")\n",
    "        else:\n",
    "            passed_checks.append(f\"✓ Row count: {individual_rows} rows match\")\n",
    "            print(f\"✓ Row count: {individual_rows} rows match\")\n",
    "        \n",
    "        # Check 2: Column names\n",
    "        # Get all unique column names from individual DataFrames\n",
    "        individual_columns = set()\n",
    "        for df in individual_dfs:\n",
    "            individual_columns.update(df.columns)\n",
    "        central_columns = set(central_df.columns)\n",
    "        \n",
    "        if individual_columns != central_columns:\n",
    "            missing_in_central = individual_columns - central_columns\n",
    "            extra_in_central = central_columns - individual_columns\n",
    "            if missing_in_central:\n",
    "                failed_checks.append(f\"Column names mismatch: Missing in central = {missing_in_central}\")\n",
    "            if extra_in_central:\n",
    "                failed_checks.append(f\"Column names mismatch: Extra in central = {extra_in_central}\")\n",
    "        else:\n",
    "            passed_checks.append(f\"✓ Column names: {len(individual_columns)} columns match\")\n",
    "            print(f\"✓ Column names: {len(individual_columns)} columns match\")\n",
    "        \n",
    "        # Check 3: Date columns - min and max\n",
    "        date_columns = [col for col in central_df.columns if 'date' in str(col).lower()]\n",
    "        if date_columns:\n",
    "            for date_col in date_columns:\n",
    "                # Get min and max from individual DataFrames\n",
    "                individual_mins = []\n",
    "                individual_maxs = []\n",
    "                for df in individual_dfs:\n",
    "                    if date_col in df.columns:\n",
    "                        col_data = pd.to_datetime(df[date_col], errors='coerce').dropna()\n",
    "                        if not col_data.empty:\n",
    "                            individual_mins.append(col_data.min())\n",
    "                            individual_maxs.append(col_data.max())\n",
    "                \n",
    "                if individual_mins and individual_maxs:\n",
    "                    individual_min = min(individual_mins)\n",
    "                    individual_max = max(individual_maxs)\n",
    "                    central_min = pd.to_datetime(central_df[date_col], errors='coerce').dropna().min()\n",
    "                    central_max = pd.to_datetime(central_df[date_col], errors='coerce').dropna().max()\n",
    "                    \n",
    "                    if pd.isna(central_min) or pd.isna(central_max):\n",
    "                        failed_checks.append(f\"Date column '{date_col}': Central has no valid dates\")\n",
    "                    else:\n",
    "                        min_match = individual_min == central_min\n",
    "                        max_match = individual_max == central_max\n",
    "                        \n",
    "                        if min_match and max_match:\n",
    "                            passed_checks.append(f\"✓ Date column '{date_col}': Min ({individual_min}) and Max ({individual_max}) match\")\n",
    "                            print(f\"✓ Date column '{date_col}': Min ({individual_min}) and Max ({individual_max}) match\")\n",
    "                        else:\n",
    "                            if not min_match:\n",
    "                                failed_checks.append(f\"Date column '{date_col}': Min mismatch - Individual = {individual_min}, Central = {central_min}\")\n",
    "                            if not max_match:\n",
    "                                failed_checks.append(f\"Date column '{date_col}': Max mismatch - Individual = {individual_max}, Central = {central_max}\")\n",
    "        else:\n",
    "            print(f\"ℹ No date columns found to validate\")\n",
    "        \n",
    "        # Check 4: Amount columns - totals and standard deviations\n",
    "        amount_columns = [col for col in central_df.columns if 'amount' in str(col).lower()]\n",
    "        if amount_columns:\n",
    "            for amount_col in amount_columns:\n",
    "                # Get totals and std dev from individual DataFrames\n",
    "                individual_totals = []\n",
    "                individual_stds = []\n",
    "                individual_values = []\n",
    "                \n",
    "                for df in individual_dfs:\n",
    "                    if amount_col in df.columns:\n",
    "                        col_data = pd.to_numeric(df[amount_col], errors='coerce').dropna()\n",
    "                        if not col_data.empty:\n",
    "                            individual_totals.append(col_data.sum())\n",
    "                            individual_stds.append(col_data.std())\n",
    "                            individual_values.extend(col_data.tolist())\n",
    "                \n",
    "                if individual_totals:\n",
    "                    individual_total = sum(individual_totals)\n",
    "                    # Calculate overall std from all individual values combined\n",
    "                    individual_std = pd.Series(individual_values).std() if individual_values else 0\n",
    "                    \n",
    "                    central_values = pd.to_numeric(central_df[amount_col], errors='coerce').dropna()\n",
    "                    if central_values.empty:\n",
    "                        failed_checks.append(f\"Amount column '{amount_col}': Central has no valid numeric values\")\n",
    "                    else:\n",
    "                        central_total = central_values.sum()\n",
    "                        central_std = central_values.std()\n",
    "                        \n",
    "                        # Use small tolerance for floating point comparison\n",
    "                        tolerance = 0.01\n",
    "                        total_match = abs(individual_total - central_total) <= tolerance\n",
    "                        std_match = abs(individual_std - central_std) <= tolerance\n",
    "                        \n",
    "                        if total_match and std_match:\n",
    "                            passed_checks.append(f\"✓ Amount column '{amount_col}': Total ({individual_total:.2f}) and Std Dev ({individual_std:.2f}) match\")\n",
    "                            print(f\"✓ Amount column '{amount_col}': Total ({individual_total:.2f}) and Std Dev ({individual_std:.2f}) match\")\n",
    "                        else:\n",
    "                            if not total_match:\n",
    "                                failed_checks.append(f\"Amount column '{amount_col}': Total mismatch - Individual = {individual_total:.2f}, Central = {central_total:.2f}\")\n",
    "                            if not std_match:\n",
    "                                failed_checks.append(f\"Amount column '{amount_col}': Std deviation mismatch - Individual = {individual_std:.2f}, Central = {central_std:.2f}\")\n",
    "        else:\n",
    "            print(f\"ℹ No amount columns found to validate\")\n",
    "        \n",
    "        return failed_checks\n",
    "    \n",
    "    # Validate Chase\n",
    "    chase_failed = validate_concatenation(chase_dataframes, chase_central, \"Chase\")\n",
    "    if chase_failed:\n",
    "        error_msg = \"Mismatch between Chase individual files and chase_central:\\n\" + \"\\n\".join(f\"  - {check}\" for check in chase_failed)\n",
    "        raise ValueError(error_msg)\n",
    "    else:\n",
    "        print(f\"✓ All Chase validation checks passed!\\n\")\n",
    "    \n",
    "    # Validate BofA\n",
    "    bofa_failed = validate_concatenation(bofa_dataframes, bofa_central, \"BofA\")\n",
    "    if bofa_failed:\n",
    "        error_msg = \"Mismatch between BofA individual files and bofa_central:\\n\" + \"\\n\".join(f\"  - {check}\" for check in bofa_failed)\n",
    "        raise ValueError(error_msg)\n",
    "    else:\n",
    "        print(f\"✓ All BofA validation checks passed!\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"Chase\": chase_files,\n",
    "        \"BofA\": bofa_files,\n",
    "        \"chase_central\": chase_central,\n",
    "        \"bofa_central\": bofa_central\n",
    "    }\n",
    "\n",
    "cc = filter_and_split_banks(dataframe_files, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3ec581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Details</th>\n",
       "      <th>Posting Date</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...</td>\n",
       "      <td>25.28</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...</td>\n",
       "      <td>100.71</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...</td>\n",
       "      <td>175.30</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...</td>\n",
       "      <td>295.55</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...</td>\n",
       "      <td>358.75</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>ORIG CO NAME:EMS                    ORIG ID:12...</td>\n",
       "      <td>770.62</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>ORIG CO NAME:EMS                    ORIG ID:12...</td>\n",
       "      <td>804.69</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>ORIG CO NAME:BANKCARD DEP           ORIG ID:20...</td>\n",
       "      <td>1567.69</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>ORIG CO NAME:BANKCARD DEP           ORIG ID:20...</td>\n",
       "      <td>1606.47</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>ORIG CO NAME:BANKCARD DEP           ORIG ID:20...</td>\n",
       "      <td>1904.29</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2247 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Details Posting Date                                        Description  \\\n",
       "0     CREDIT   2025-10-06  ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...   \n",
       "1     CREDIT   2025-10-06  ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...   \n",
       "2     CREDIT   2025-10-06  ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...   \n",
       "3     CREDIT   2025-10-06  ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...   \n",
       "4     CREDIT   2025-10-06  ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...   \n",
       "...      ...          ...                                                ...   \n",
       "2242  CREDIT   2025-09-02  ORIG CO NAME:EMS                    ORIG ID:12...   \n",
       "2243  CREDIT   2025-09-02  ORIG CO NAME:EMS                    ORIG ID:12...   \n",
       "2244  CREDIT   2025-09-02  ORIG CO NAME:BANKCARD DEP           ORIG ID:20...   \n",
       "2245  CREDIT   2025-09-02  ORIG CO NAME:BANKCARD DEP           ORIG ID:20...   \n",
       "2246  CREDIT   2025-09-02  ORIG CO NAME:BANKCARD DEP           ORIG ID:20...   \n",
       "\n",
       "       Amount        Type  \n",
       "0       25.28  ACH_CREDIT  \n",
       "1      100.71  ACH_CREDIT  \n",
       "2      175.30  ACH_CREDIT  \n",
       "3      295.55  ACH_CREDIT  \n",
       "4      358.75  ACH_CREDIT  \n",
       "...       ...         ...  \n",
       "2242   770.62  ACH_CREDIT  \n",
       "2243   804.69  ACH_CREDIT  \n",
       "2244  1567.69  ACH_CREDIT  \n",
       "2245  1606.47  ACH_CREDIT  \n",
       "2246  1904.29  ACH_CREDIT  \n",
       "\n",
       "[2247 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc[\"chase_central\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad9d9320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>BANKCARD DES:MTOT DISC ID:422899490017607 INDN...</td>\n",
       "      <td>-402.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-16</td>\n",
       "      <td>LQ MERCHANT DES:ADJUSTMENT ID:584600000388686 ...</td>\n",
       "      <td>-335.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-16</td>\n",
       "      <td>LQ MERCHANT DES:ADJUSTMENT ID:584600000388736 ...</td>\n",
       "      <td>-335.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-02</td>\n",
       "      <td>LQ MERCHANT DES:MERCH FEES ID:584600000388744 ...</td>\n",
       "      <td>-310.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-02</td>\n",
       "      <td>LQ MERCHANT DES:MERCH FEES ID:584600000388686 ...</td>\n",
       "      <td>-310.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>RETURN OF POSTED CHECK / ITEM (RECEIVED ON 10-...</td>\n",
       "      <td>310.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>2025-10-17</td>\n",
       "      <td>BANKCARD DES:MTOT DEP ID:422899490017605 INDN:...</td>\n",
       "      <td>368.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>2025-10-17</td>\n",
       "      <td>BANKCARD DES:MTOT DEP ID:422899490017608 INDN:...</td>\n",
       "      <td>427.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>2025-07-07</td>\n",
       "      <td>WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...</td>\n",
       "      <td>1376.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2025-07-14</td>\n",
       "      <td>WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...</td>\n",
       "      <td>1451.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date                                        Description   Amount\n",
       "0   2025-09-02  BANKCARD DES:MTOT DISC ID:422899490017607 INDN...  -402.02\n",
       "1   2025-07-16  LQ MERCHANT DES:ADJUSTMENT ID:584600000388686 ...  -335.00\n",
       "2   2025-07-16  LQ MERCHANT DES:ADJUSTMENT ID:584600000388736 ...  -335.00\n",
       "3   2025-07-02  LQ MERCHANT DES:MERCH FEES ID:584600000388744 ...  -310.00\n",
       "4   2025-07-02  LQ MERCHANT DES:MERCH FEES ID:584600000388686 ...  -310.00\n",
       "..         ...                                                ...      ...\n",
       "375 2025-10-03  RETURN OF POSTED CHECK / ITEM (RECEIVED ON 10-...   310.00\n",
       "376 2025-10-17  BANKCARD DES:MTOT DEP ID:422899490017605 INDN:...   368.94\n",
       "377 2025-10-17  BANKCARD DES:MTOT DEP ID:422899490017608 INDN:...   427.28\n",
       "378 2025-07-07  WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...  1376.90\n",
       "379 2025-07-14  WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...  1451.90\n",
       "\n",
       "[380 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc[\"bofa_central\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b927da34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validating BofA Standardization ===\n",
      "✓ Row count: 380 rows match\n",
      "✓ Column structure: Matches chase_central exactly (5 columns)\n",
      "✓ Details column: All values are DEBIT or CREDIT\n",
      "✓ Details-Amount consistency: Details correctly match Amount sign\n",
      "✓ Posting Date: Matches original Date column\n",
      "✓ Description: Matches original Description column\n",
      "✓ Amount: Matches original Amount column\n",
      "✓ Type column: All values have ACH_ prefix\n",
      "✓ Type-Details consistency: Type correctly matches Details with ACH_ prefix\n",
      "✓ All BofA standardization checks passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def bofa_standardizer(bofa_central: Optional[pd.DataFrame] = None, chase_central: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    BofA Standardizer Function\n",
    "    \n",
    "    This function transforms the bofa_central DataFrame to match the format and structure of chase_central.\n",
    "    Column mappings:\n",
    "    - Details: Based on Amount column (negative = DEBIT, positive = CREDIT)\n",
    "    - Posting Date: From Date column in bofa\n",
    "    - Description: Same as Description column\n",
    "    - Amount: Same as Amount column\n",
    "    - Type: From Details column with \"ACH_\" prefix (e.g., ACH_DEBIT, ACH_CREDIT)\n",
    "    \n",
    "    The output DataFrame will have the exact same column names and order as chase_central.\n",
    "    Includes robust validation checks to verify all transformations.\n",
    "    \n",
    "    Args:\n",
    "        bofa_central: BofA central DataFrame (defaults to global bofa_central if not provided)\n",
    "        chase_central: Chase central DataFrame (defaults to global chase_central if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Standardized BofA DataFrame matching Chase format with same column names\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If validation checks fail, listing which checks failed\n",
    "    \"\"\"\n",
    "    # Access from global scope if not provided\n",
    "    if bofa_central is None:\n",
    "        try:\n",
    "            bofa_central = globals().get('cc', {}).get('bofa_central')\n",
    "            if bofa_central is None:\n",
    "                raise ValueError(\"bofa_central not found. Please provide it as parameter or ensure 'cc' dictionary exists.\")\n",
    "        except:\n",
    "            raise ValueError(\"bofa_central not found. Please provide it as parameter.\")\n",
    "    \n",
    "    if chase_central is None:\n",
    "        try:\n",
    "            chase_central = globals().get('cc', {}).get('chase_central')\n",
    "            if chase_central is None:\n",
    "                raise ValueError(\"chase_central not found. Please provide it as parameter or ensure 'cc' dictionary exists.\")\n",
    "        except:\n",
    "            raise ValueError(\"chase_central not found. Please provide it as parameter.\")\n",
    "    \n",
    "    if bofa_central.empty:\n",
    "        raise ValueError(\"bofa_central is empty, cannot standardize\")\n",
    "    \n",
    "    if chase_central.empty:\n",
    "        raise ValueError(\"chase_central is empty, cannot use as template\")\n",
    "    \n",
    "    # Get column names from chase_central (use exact names and order)\n",
    "    chase_columns = list(chase_central.columns)\n",
    "    \n",
    "    # Find column mappings in chase_central (case-insensitive matching)\n",
    "    details_col = None\n",
    "    posting_date_col = None\n",
    "    description_col = None\n",
    "    amount_col_chase = None\n",
    "    type_col = None\n",
    "    \n",
    "    for col in chase_columns:\n",
    "        col_lower = col.lower().strip()\n",
    "        if col_lower == 'details':\n",
    "            details_col = col\n",
    "        elif 'posting' in col_lower and 'date' in col_lower:\n",
    "            posting_date_col = col\n",
    "        elif col_lower == 'description':\n",
    "            description_col = col\n",
    "        elif col_lower == 'amount':\n",
    "            amount_col_chase = col\n",
    "        elif col_lower == 'type':\n",
    "            type_col = col\n",
    "    \n",
    "    # Validate we found all required Chase columns\n",
    "    missing_chase_cols = []\n",
    "    if not details_col:\n",
    "        missing_chase_cols.append(\"Details\")\n",
    "    if not posting_date_col:\n",
    "        missing_chase_cols.append(\"Posting Date\")\n",
    "    if not description_col:\n",
    "        missing_chase_cols.append(\"Description\")\n",
    "    if not amount_col_chase:\n",
    "        missing_chase_cols.append(\"Amount\")\n",
    "    if not type_col:\n",
    "        missing_chase_cols.append(\"Type\")\n",
    "    \n",
    "    if missing_chase_cols:\n",
    "        raise ValueError(f\"Missing required columns in chase_central: {missing_chase_cols}\")\n",
    "    \n",
    "    # Get bofa column names (case-insensitive)\n",
    "    bofa_columns_lower = [col.lower().strip() for col in bofa_central.columns]\n",
    "    bofa_date_col = None\n",
    "    bofa_description_col = None\n",
    "    bofa_amount_col = None\n",
    "    \n",
    "    for col in bofa_central.columns:\n",
    "        col_lower = col.lower().strip()\n",
    "        if col_lower == 'date':\n",
    "            bofa_date_col = col\n",
    "        elif col_lower == 'description':\n",
    "            bofa_description_col = col\n",
    "        elif col_lower == 'amount':\n",
    "            bofa_amount_col = col\n",
    "    \n",
    "    # Validate required columns exist in bofa\n",
    "    missing_bofa_cols = []\n",
    "    if not bofa_date_col:\n",
    "        missing_bofa_cols.append(\"Date\")\n",
    "    if not bofa_description_col:\n",
    "        missing_bofa_cols.append(\"Description\")\n",
    "    if not bofa_amount_col:\n",
    "        missing_bofa_cols.append(\"Amount\")\n",
    "    \n",
    "    if missing_bofa_cols:\n",
    "        raise ValueError(f\"Missing required columns in bofa_central: {missing_bofa_cols}\")\n",
    "    \n",
    "    # Create new DataFrame with exact same structure as chase_central\n",
    "    standardized_df = pd.DataFrame(index=bofa_central.index)\n",
    "    \n",
    "    # Map columns in the same order as chase_central\n",
    "    for col in chase_columns:\n",
    "        if col == details_col:\n",
    "            # Details column: Based on Amount (negative = DEBIT, positive = CREDIT)\n",
    "            standardized_df[col] = bofa_central[bofa_amount_col].apply(\n",
    "                lambda x: \"DEBIT\" if pd.notna(x) and float(x) < 0 else \"CREDIT\" if pd.notna(x) else None\n",
    "            )\n",
    "        elif col == posting_date_col:\n",
    "            # Posting Date column: From Date column\n",
    "            standardized_df[col] = bofa_central[bofa_date_col]\n",
    "        elif col == description_col:\n",
    "            # Description column: Same as Description\n",
    "            standardized_df[col] = bofa_central[bofa_description_col]\n",
    "        elif col == amount_col_chase:\n",
    "            # Amount column: Same as Amount\n",
    "            standardized_df[col] = bofa_central[bofa_amount_col]\n",
    "        elif col == type_col:\n",
    "            # Type column: Based on Details with \"ACH_\" prefix\n",
    "            details_values = bofa_central[bofa_amount_col].apply(\n",
    "                lambda x: \"DEBIT\" if pd.notna(x) and float(x) < 0 else \"CREDIT\" if pd.notna(x) else None\n",
    "            )\n",
    "            standardized_df[col] = details_values.apply(\n",
    "                lambda x: f\"ACH_{x}\" if pd.notna(x) else None\n",
    "            )\n",
    "        else:\n",
    "            # For any other columns in chase_central, fill with NaN\n",
    "            standardized_df[col] = None\n",
    "    \n",
    "    # Ensure column order matches chase_central exactly\n",
    "    standardized_df = standardized_df[chase_columns]\n",
    "    \n",
    "    # Reset index to match chase format\n",
    "    standardized_df = standardized_df.reset_index(drop=True)\n",
    "    \n",
    "    # Robust validation checks\n",
    "    print(\"\\n=== Validating BofA Standardization ===\")\n",
    "    failed_checks = []\n",
    "    \n",
    "    # Check 1: Row count matches\n",
    "    if len(standardized_df) != len(bofa_central):\n",
    "        failed_checks.append(f\"Row count mismatch: Original = {len(bofa_central)}, Standardized = {len(standardized_df)}\")\n",
    "    else:\n",
    "        print(f\"✓ Row count: {len(standardized_df)} rows match\")\n",
    "    \n",
    "    # Check 2: Column names and order match chase_central exactly\n",
    "    if list(standardized_df.columns) != chase_columns:\n",
    "        failed_checks.append(f\"Column names/order mismatch: Expected {chase_columns}, Got {list(standardized_df.columns)}\")\n",
    "    else:\n",
    "        print(f\"✓ Column structure: Matches chase_central exactly ({len(chase_columns)} columns)\")\n",
    "    \n",
    "    # Check 3: Details column values are DEBIT or CREDIT\n",
    "    details_values = standardized_df[details_col].dropna().unique()\n",
    "    valid_details = set(['DEBIT', 'CREDIT'])\n",
    "    if not set(details_values).issubset(valid_details):\n",
    "        invalid = set(details_values) - valid_details\n",
    "        failed_checks.append(f\"Details column has invalid values: {invalid}\")\n",
    "    else:\n",
    "        print(f\"✓ Details column: All values are DEBIT or CREDIT\")\n",
    "    \n",
    "    # Check 4: Details matches Amount sign (negative = DEBIT, positive = CREDIT)\n",
    "    amount_details_match = True\n",
    "    for idx in standardized_df.index:\n",
    "        amount_val = standardized_df.loc[idx, amount_col_chase]\n",
    "        details_val = standardized_df.loc[idx, details_col]\n",
    "        if pd.notna(amount_val) and pd.notna(details_val):\n",
    "            expected_detail = \"DEBIT\" if amount_val < 0 else \"CREDIT\"\n",
    "            if details_val != expected_detail:\n",
    "                amount_details_match = False\n",
    "                break\n",
    "    \n",
    "    if not amount_details_match:\n",
    "        failed_checks.append(\"Details column does not match Amount sign (negative should be DEBIT, positive should be CREDIT)\")\n",
    "    else:\n",
    "        print(f\"✓ Details-Amount consistency: Details correctly match Amount sign\")\n",
    "    \n",
    "    # Check 5: Posting Date matches original Date\n",
    "    date_match = standardized_df[posting_date_col].equals(bofa_central[bofa_date_col].reset_index(drop=True))\n",
    "    if not date_match:\n",
    "        # Check if they're equal after conversion\n",
    "        date_match = (\n",
    "            pd.to_datetime(standardized_df[posting_date_col], errors='coerce')\n",
    "            .reset_index(drop=True)\n",
    "            .equals(pd.to_datetime(bofa_central[bofa_date_col], errors='coerce').reset_index(drop=True))\n",
    "        )\n",
    "    if not date_match:\n",
    "        failed_checks.append(\"Posting Date does not match original Date column\")\n",
    "    else:\n",
    "        print(f\"✓ Posting Date: Matches original Date column\")\n",
    "    \n",
    "    # Check 6: Description matches original Description\n",
    "    desc_match = standardized_df[description_col].equals(bofa_central[bofa_description_col].reset_index(drop=True))\n",
    "    if not desc_match:\n",
    "        failed_checks.append(\"Description does not match original Description column\")\n",
    "    else:\n",
    "        print(f\"✓ Description: Matches original Description column\")\n",
    "    \n",
    "    # Check 7: Amount matches original Amount\n",
    "    amount_match = standardized_df[amount_col_chase].equals(bofa_central[bofa_amount_col].reset_index(drop=True))\n",
    "    if not amount_match:\n",
    "        # Check with tolerance for floating point\n",
    "        amount_diff = abs(standardized_df[amount_col_chase].reset_index(drop=True) - bofa_central[bofa_amount_col].reset_index(drop=True)).max()\n",
    "        if amount_diff > 0.01:\n",
    "            failed_checks.append(f\"Amount does not match original Amount column (max diff: {amount_diff})\")\n",
    "        else:\n",
    "            print(f\"✓ Amount: Matches original Amount column (within tolerance)\")\n",
    "    else:\n",
    "        print(f\"✓ Amount: Matches original Amount column\")\n",
    "    \n",
    "    # Check 8: Type column has ACH_ prefix\n",
    "    type_values = standardized_df[type_col].dropna().unique()\n",
    "    all_have_prefix = all(str(val).startswith('ACH_') for val in type_values if pd.notna(val))\n",
    "    if not all_have_prefix:\n",
    "        invalid_types = [val for val in type_values if not str(val).startswith('ACH_')]\n",
    "        failed_checks.append(f\"Type column has values without ACH_ prefix: {invalid_types}\")\n",
    "    else:\n",
    "        print(f\"✓ Type column: All values have ACH_ prefix\")\n",
    "    \n",
    "    # Check 9: Type matches Details with ACH_ prefix\n",
    "    type_details_match = True\n",
    "    for idx in standardized_df.index:\n",
    "        details_val = standardized_df.loc[idx, details_col]\n",
    "        type_val = standardized_df.loc[idx, type_col]\n",
    "        if pd.notna(details_val) and pd.notna(type_val):\n",
    "            expected_type = f\"ACH_{details_val}\"\n",
    "            if type_val != expected_type:\n",
    "                type_details_match = False\n",
    "                break\n",
    "    \n",
    "    if not type_details_match:\n",
    "        failed_checks.append(\"Type column does not match Details with ACH_ prefix\")\n",
    "    else:\n",
    "        print(f\"✓ Type-Details consistency: Type correctly matches Details with ACH_ prefix\")\n",
    "    \n",
    "    # Raise error if any checks failed\n",
    "    if failed_checks:\n",
    "        error_msg = \"BofA standardization validation failed:\\n\" + \"\\n\".join(f\"  - {check}\" for check in failed_checks)\n",
    "        raise ValueError(error_msg)\n",
    "    else:\n",
    "        print(f\"✓ All BofA standardization checks passed!\\n\")\n",
    "    \n",
    "    return standardized_df\n",
    "\n",
    "\n",
    "bofa_central_standardized = bofa_standardizer(cc[\"bofa_central\"], cc[\"chase_central\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca76175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Details</th>\n",
       "      <th>Posting Date</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>BANKCARD DES:MTOT DISC ID:422899490017607 INDN...</td>\n",
       "      <td>-402.02</td>\n",
       "      <td>ACH_DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>2025-07-16</td>\n",
       "      <td>LQ MERCHANT DES:ADJUSTMENT ID:584600000388686 ...</td>\n",
       "      <td>-335.00</td>\n",
       "      <td>ACH_DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>2025-07-16</td>\n",
       "      <td>LQ MERCHANT DES:ADJUSTMENT ID:584600000388736 ...</td>\n",
       "      <td>-335.00</td>\n",
       "      <td>ACH_DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>2025-07-02</td>\n",
       "      <td>LQ MERCHANT DES:MERCH FEES ID:584600000388744 ...</td>\n",
       "      <td>-310.00</td>\n",
       "      <td>ACH_DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>2025-07-02</td>\n",
       "      <td>LQ MERCHANT DES:MERCH FEES ID:584600000388686 ...</td>\n",
       "      <td>-310.00</td>\n",
       "      <td>ACH_DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>RETURN OF POSTED CHECK / ITEM (RECEIVED ON 10-...</td>\n",
       "      <td>310.00</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-17</td>\n",
       "      <td>BANKCARD DES:MTOT DEP ID:422899490017605 INDN:...</td>\n",
       "      <td>368.94</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-17</td>\n",
       "      <td>BANKCARD DES:MTOT DEP ID:422899490017608 INDN:...</td>\n",
       "      <td>427.28</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-07-07</td>\n",
       "      <td>WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...</td>\n",
       "      <td>1376.90</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-07-14</td>\n",
       "      <td>WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...</td>\n",
       "      <td>1451.90</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Details Posting Date                                        Description  \\\n",
       "0     DEBIT   2025-09-02  BANKCARD DES:MTOT DISC ID:422899490017607 INDN...   \n",
       "1     DEBIT   2025-07-16  LQ MERCHANT DES:ADJUSTMENT ID:584600000388686 ...   \n",
       "2     DEBIT   2025-07-16  LQ MERCHANT DES:ADJUSTMENT ID:584600000388736 ...   \n",
       "3     DEBIT   2025-07-02  LQ MERCHANT DES:MERCH FEES ID:584600000388744 ...   \n",
       "4     DEBIT   2025-07-02  LQ MERCHANT DES:MERCH FEES ID:584600000388686 ...   \n",
       "..      ...          ...                                                ...   \n",
       "375  CREDIT   2025-10-03  RETURN OF POSTED CHECK / ITEM (RECEIVED ON 10-...   \n",
       "376  CREDIT   2025-10-17  BANKCARD DES:MTOT DEP ID:422899490017605 INDN:...   \n",
       "377  CREDIT   2025-10-17  BANKCARD DES:MTOT DEP ID:422899490017608 INDN:...   \n",
       "378  CREDIT   2025-07-07  WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...   \n",
       "379  CREDIT   2025-07-14  WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...   \n",
       "\n",
       "      Amount        Type  \n",
       "0    -402.02   ACH_DEBIT  \n",
       "1    -335.00   ACH_DEBIT  \n",
       "2    -335.00   ACH_DEBIT  \n",
       "3    -310.00   ACH_DEBIT  \n",
       "4    -310.00   ACH_DEBIT  \n",
       "..       ...         ...  \n",
       "375   310.00  ACH_CREDIT  \n",
       "376   368.94  ACH_CREDIT  \n",
       "377   427.28  ACH_CREDIT  \n",
       "378  1376.90  ACH_CREDIT  \n",
       "379  1451.90  ACH_CREDIT  \n",
       "\n",
       "[380 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bofa_central_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a19b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Central DataFrame Created ===\n",
      "✓ Chase rows: 2247\n",
      "✓ BofA rows: 380\n",
      "✓ Total rows in central_df: 2627\n",
      "✓ Columns: ['Details', 'Posting Date', 'Description', 'Amount', 'Type', 'Bank Name']\n",
      "✓ Total columns: 6\n",
      "✓ Row count validation passed: 2627 rows\n",
      "✓ Bank Name column validation passed: 2247 CHASE rows, 380 BofA rows\n",
      "\n",
      "✓ Central DataFrame creation complete!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Details</th>\n",
       "      <th>Posting Date</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Type</th>\n",
       "      <th>Bank Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...</td>\n",
       "      <td>25.28</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "      <td>CHASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...</td>\n",
       "      <td>100.71</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "      <td>CHASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...</td>\n",
       "      <td>175.30</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "      <td>CHASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...</td>\n",
       "      <td>295.55</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "      <td>CHASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...</td>\n",
       "      <td>358.75</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "      <td>CHASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>RETURN OF POSTED CHECK / ITEM (RECEIVED ON 10-...</td>\n",
       "      <td>310.00</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "      <td>BofA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-17</td>\n",
       "      <td>BANKCARD DES:MTOT DEP ID:422899490017605 INDN:...</td>\n",
       "      <td>368.94</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "      <td>BofA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-10-17</td>\n",
       "      <td>BANKCARD DES:MTOT DEP ID:422899490017608 INDN:...</td>\n",
       "      <td>427.28</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "      <td>BofA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-07-07</td>\n",
       "      <td>WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...</td>\n",
       "      <td>1376.90</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "      <td>BofA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>CREDIT</td>\n",
       "      <td>2025-07-14</td>\n",
       "      <td>WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...</td>\n",
       "      <td>1451.90</td>\n",
       "      <td>ACH_CREDIT</td>\n",
       "      <td>BofA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2627 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Details Posting Date                                        Description  \\\n",
       "0     CREDIT   2025-10-06  ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...   \n",
       "1     CREDIT   2025-10-06  ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...   \n",
       "2     CREDIT   2025-10-06  ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...   \n",
       "3     CREDIT   2025-10-06  ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...   \n",
       "4     CREDIT   2025-10-06  ORIG CO NAME:LQ MERCHANT      CO ENTRY DESCR:D...   \n",
       "...      ...          ...                                                ...   \n",
       "2622  CREDIT   2025-10-03  RETURN OF POSTED CHECK / ITEM (RECEIVED ON 10-...   \n",
       "2623  CREDIT   2025-10-17  BANKCARD DES:MTOT DEP ID:422899490017605 INDN:...   \n",
       "2624  CREDIT   2025-10-17  BANKCARD DES:MTOT DEP ID:422899490017608 INDN:...   \n",
       "2625  CREDIT   2025-07-07  WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...   \n",
       "2626  CREDIT   2025-07-14  WISE US INC DES:SMART IMPA ID:SMART IMPA INDN:...   \n",
       "\n",
       "       Amount        Type Bank Name  \n",
       "0       25.28  ACH_CREDIT     CHASE  \n",
       "1      100.71  ACH_CREDIT     CHASE  \n",
       "2      175.30  ACH_CREDIT     CHASE  \n",
       "3      295.55  ACH_CREDIT     CHASE  \n",
       "4      358.75  ACH_CREDIT     CHASE  \n",
       "...       ...         ...       ...  \n",
       "2622   310.00  ACH_CREDIT      BofA  \n",
       "2623   368.94  ACH_CREDIT      BofA  \n",
       "2624   427.28  ACH_CREDIT      BofA  \n",
       "2625  1376.90  ACH_CREDIT      BofA  \n",
       "2626  1451.90  ACH_CREDIT      BofA  \n",
       "\n",
       "[2627 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_central_df(chase_central: Optional[pd.DataFrame] = None, bofa_central_standardized: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create Central DataFrame Function\n",
    "    \n",
    "    This function concatenates chase_central and bofa_central_standardized DataFrames\n",
    "    into a single unified DataFrame called central_df.\n",
    "    Adds a \"Bank Name\" column to identify the source: \"CHASE\" for Chase rows, \"BofA\" for BofA rows.\n",
    "    \n",
    "    Args:\n",
    "        chase_central: Chase central DataFrame (defaults to global chase_central if not provided)\n",
    "        bofa_central_standardized: Standardized BofA DataFrame (defaults to global bofa_central_standardized if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame containing both Chase and BofA data with Bank Name column\n",
    "    \"\"\"\n",
    "    # Access from global scope if not provided\n",
    "    if chase_central is None:\n",
    "        try:\n",
    "            chase_central = globals().get('cc', {}).get('chase_central')\n",
    "            if chase_central is None:\n",
    "                raise ValueError(\"chase_central not found. Please provide it as parameter or ensure 'cc' dictionary exists.\")\n",
    "        except:\n",
    "            raise ValueError(\"chase_central not found. Please provide it as parameter.\")\n",
    "    \n",
    "    if bofa_central_standardized is None:\n",
    "        try:\n",
    "            bofa_central_standardized = globals().get('bofa_central_standardized')\n",
    "            if bofa_central_standardized is None:\n",
    "                raise ValueError(\"bofa_central_standardized not found. Please provide it as parameter or ensure it exists.\")\n",
    "        except:\n",
    "            raise ValueError(\"bofa_central_standardized not found. Please provide it as parameter.\")\n",
    "    \n",
    "    if chase_central.empty:\n",
    "        raise ValueError(\"chase_central is empty, cannot concatenate\")\n",
    "    \n",
    "    if bofa_central_standardized.empty:\n",
    "        raise ValueError(\"bofa_central_standardized is empty, cannot concatenate\")\n",
    "    \n",
    "    # Validate that both DataFrames have the same columns (excluding Bank Name which we'll add)\n",
    "    chase_columns = set(chase_central.columns)\n",
    "    bofa_columns = set(bofa_central_standardized.columns)\n",
    "    \n",
    "    if chase_columns != bofa_columns:\n",
    "        missing_in_bofa = chase_columns - bofa_columns\n",
    "        missing_in_chase = bofa_columns - chase_columns\n",
    "        error_msg = \"Column mismatch between chase_central and bofa_central_standardized:\\n\"\n",
    "        if missing_in_bofa:\n",
    "            error_msg += f\"  Missing in bofa_central_standardized: {missing_in_bofa}\\n\"\n",
    "        if missing_in_chase:\n",
    "            error_msg += f\"  Missing in chase_central: {missing_in_chase}\"\n",
    "        raise ValueError(error_msg)\n",
    "    \n",
    "    # Create copies to avoid modifying originals\n",
    "    chase_df = chase_central.copy()\n",
    "    bofa_df = bofa_central_standardized.copy()\n",
    "    \n",
    "    # Add Bank Name column to each DataFrame\n",
    "    chase_df['Bank Name'] = 'CHASE'\n",
    "    bofa_df['Bank Name'] = 'BofA'\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    central_df = pd.concat([chase_df, bofa_df], ignore_index=True)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== Central DataFrame Created ===\")\n",
    "    print(f\"✓ Chase rows: {len(chase_central)}\")\n",
    "    print(f\"✓ BofA rows: {len(bofa_central_standardized)}\")\n",
    "    print(f\"✓ Total rows in central_df: {len(central_df)}\")\n",
    "    print(f\"✓ Columns: {list(central_df.columns)}\")\n",
    "    print(f\"✓ Total columns: {len(central_df.columns)}\")\n",
    "    \n",
    "    # Validate row count\n",
    "    expected_rows = len(chase_central) + len(bofa_central_standardized)\n",
    "    if len(central_df) != expected_rows:\n",
    "        raise ValueError(\n",
    "            f\"Row count mismatch: Expected {expected_rows} rows, got {len(central_df)} rows\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"✓ Row count validation passed: {expected_rows} rows\")\n",
    "    \n",
    "    # Validate Bank Name column\n",
    "    chase_count = (central_df['Bank Name'] == 'CHASE').sum()\n",
    "    bofa_count = (central_df['Bank Name'] == 'BofA').sum()\n",
    "    if chase_count == len(chase_central) and bofa_count == len(bofa_central_standardized):\n",
    "        print(f\"✓ Bank Name column validation passed: {chase_count} CHASE rows, {bofa_count} BofA rows\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Bank Name column mismatch: Expected {len(chase_central)} CHASE and {len(bofa_central_standardized)} BofA, \"\n",
    "            f\"got {chase_count} CHASE and {bofa_count} BofA\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n✓ Central DataFrame creation complete!\\n\")\n",
    "    \n",
    "    return central_df\n",
    "\n",
    "central_df = create_central_df(cc[\"chase_central\"], bofa_central_standardized)\n",
    "central_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7ed8a",
   "metadata": {},
   "source": [
    "## **3) MID Extraction & Enrichment - Transaction Classification Pipeline**\n",
    "\n",
    "This section extracts Merchant IDs (MIDs) and credit charge types from transaction descriptions, then enriches each transaction with GWID, Processor, and Corp information through fuzzy matching.\n",
    "\n",
    "### **What this section does:**\n",
    "\n",
    "1. **Extract metadata from descriptions** (regex-based pattern matching):\n",
    "   - **MID (Merchant ID)**: Extracts digits between `INDID` and (`INDNAME` or `ORIGID`) tokens\n",
    "   - **Credit charge type**: Extracts letters between `COENTRYDESCR` and `SEC` tokens (e.g., `merchdep`, `deposit`, `tpresrel`)\n",
    "   - Creates `clean_description` (lowercase, no spaces/special chars), `midid`, and `credit_charge` columns\n",
    "\n",
    "2. **Fuzzy match MIDs to canonical reference** (`match_mid_score` function):\n",
    "   - Normalizes extracted MIDs to digits-only\n",
    "   - Uses RapidFuzz `partial_ratio` with 85% score cutoff\n",
    "   - Matches against GWID reference DataFrame to find canonical MID\n",
    "   - Returns best match + confidence score\n",
    "\n",
    "3. **Hierarchical lookups** (MID → GWID → Processor → Corp):\n",
    "   - **MID → GWID**: Maps matched MID to Gateway ID\n",
    "   - **GWID → Processor**: Maps Gateway ID to processor name (e.g., \"Luqra Evolve\", \"EMS\")\n",
    "   - **GWID/MID → Corp**: Maps to corporate entity when available\n",
    "   - Appends `matched_mid`, `gwid`, `processor`, and `corp` columns to each transaction\n",
    "\n",
    "4. **Diagnostics & validation**:\n",
    "   - Reports rows with missing MIDs and categorizes failure reasons:\n",
    "     - Missing `INDID` token\n",
    "     - Missing end token (`INDNAME`/`ORIGID`)\n",
    "     - Invalid extraction window/regex miss\n",
    "   - Logs low-confidence matches (score < 85%)\n",
    "   - Provides sample output for inspection\n",
    "\n",
    "5. **Output**:\n",
    "   - Returns enriched DataFrame (`result_df`) with columns:\n",
    "     - Original: `Details`, `Posting Date`, `Description`, `Amount`, `Type`, `Bank Name`\n",
    "     - Added: `clean_description`, `midid`, `credit_charge`, `matched_mid`, `gwid`, `processor`, `corp`\n",
    "   - Ready for aggregation into pivot tables by GWID, Processor, or Corp\n",
    "\n",
    "### **Key functions:**\n",
    "- `single_pass_extract_mid_credit()`: Vectorized extraction + fuzzy matching in one pass\n",
    "- `match_mid_score()`: Fuzzy MID matcher with caching (from Section 1 imports)\n",
    "\n",
    "### **Dependencies:**\n",
    "- Requires `gwids_df` reference DataFrame with columns: `mid`, `gwid`, `processor`, `corp`\n",
    "- Uses `rapidfuzz` library for fuzzy string matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838dceca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff752658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62179603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f8329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12a4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428398bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "420d0707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ API key NOT found!\n",
      "\n",
      "To set it properly:\n",
      "1. Close this Jupyter notebook\n",
      "2. In PowerShell, run: $env:ANTHROPIC_API_KEY = 'your-key-here'\n",
      "3. Start Jupyter from that same PowerShell window: jupyter notebook\n",
      "\n",
      "OR use Option 2 below (python-dotenv with .env file)\n"
     ]
    }
   ],
   "source": [
    "# Check if API key is set (from environment variable set before starting Jupyter)\n",
    "# Do NOT set it here - it should be set in your PowerShell session before starting Jupyter\n",
    "api_key_check = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if api_key_check:\n",
    "    print(f\"✓ API key found in environment\")\n",
    "else:\n",
    "    print(\"⚠ API key NOT found!\")\n",
    "    print(\"\\nTo set it properly:\")\n",
    "    print(\"1. Close this Jupyter notebook\")\n",
    "    print(\"2. In PowerShell, run: $env:ANTHROPIC_API_KEY = 'your-key-here'\")\n",
    "    print(\"3. Start Jupyter from that same PowerShell window: jupyter notebook\")\n",
    "    print(\"\\nOR use Option 2 below (python-dotenv with .env file)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e29f46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ No .env file found or API key not in .env\n",
      "Create a .env file with: ANTHROPIC_API_KEY=your-key-here\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Use python-dotenv to load from .env file (recommended for development)\n",
    "# First install: pip install python-dotenv\n",
    "# Then create a .env file in your project root with: ANTHROPIC_API_KEY=your-key-here\n",
    "# Make sure .env is in .gitignore!\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # Loads variables from .env file\n",
    "    \n",
    "    # Check again\n",
    "    if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "        print(\"✓ API key loaded from .env file\")\n",
    "    else:\n",
    "        print(\"⚠ No .env file found or API key not in .env\")\n",
    "        print(\"Create a .env file with: ANTHROPIC_API_KEY=your-key-here\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    print(\"Or use Option 1 (set env var before starting Jupyter)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db2efe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import base64\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def detect_bank_from_screenshot(excel_file_path: str, api_key: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Take screenshot of Excel file's first few rows, send to Claude Haiku.\n",
    "    Returns: bank name (e.g., \"Chase\", \"Bank of America\")\n",
    "    \n",
    "    Args:\n",
    "        excel_file_path: Path to the Excel file to analyze\n",
    "        api_key: Anthropic API key. If not provided, will try to read from \n",
    "                 ANTHROPIC_API_KEY environment variable.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If no API key is provided and ANTHROPIC_API_KEY env var is not set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get API key from parameter or environment variable\n",
    "    if api_key is None:\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\n",
    "                \"API key not provided. Either pass it as a parameter or set the \"\n",
    "                \"ANTHROPIC_API_KEY environment variable.\"\n",
    "            )\n",
    "    \n",
    "    client = anthropic.Anthropic(api_key=api_key)\n",
    "    \n",
    "    # Read first 10 rows as preview\n",
    "    df_preview = pd.read_excel(excel_file_path, nrows=10)\n",
    "    preview_text = df_preview.to_string()\n",
    "    \n",
    "    prompt = f\"\"\"Identify the bank from this statement preview. \n",
    "\n",
    "RULES:\n",
    "- If columns include \"Posting Date\", \"Description\", \"Amount\", \"Type\", \"Balance\" → Chase\n",
    "- If columns include \"Posted Date\", \"Payee\", \"Address\" → Bank of America  \n",
    "- If columns include \"Date\", \"Check Number\", \"Description\", \"Withdrawal\", \"Deposit\" → Wells Fargo\n",
    "- If you see \"CHASE\" or \"JPMorgan\" anywhere → Chase\n",
    "- If you see \"Bank of America\" or \"BofA\" → Bank of America\n",
    "\n",
    "Preview:\n",
    "{preview_text}\n",
    "\n",
    "Respond with ONLY the bank name, nothing else.\"\"\"\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        max_tokens=20,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return message.content[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
